{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "festa_demo_header"
   },
   "source": [
    "# FESTA Framework Demo: LVLM Consistency Assessment\n",
    "\n",
    "**Paper**: [FESTA: Towards understanding the failure points in LVLMs](https://arxiv.org/pdf/2410.23499.pdf)\n",
    "\n",
    "**GitHub**: https://github.com/iiscleap/mllm-uncertainty-estimation/\n",
    "\n",
    "This notebook demonstrates the FESTA framework's assessment of Large Vision-Language Models (LVLMs) consistency using **real examples and failure cases from the BLINK dataset**.\n",
    "\n",
    "FESTA evaluates two types of consistency:\n",
    "1. **Equivalent Input Assessment**: Same semantic meaning, different phrasing\n",
    "2. **Complementary Input Assessment**: Opposite/negated meanings requiring different responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision transformers pillow requests accelerate bitsandbytes\n",
    "!pip install git+https://github.com/LLaVA-VL/LLaVA-NeXT.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LVLM Model (LLaVA-NeXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LLaVA-NeXT model\n",
    "model_id = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "\n",
    "processor = LlavaNextProcessor.from_pretrained(model_id)\n",
    "model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.float16, \n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"LLaVA-NeXT model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(image, question, max_length=100):\n",
    "    \"\"\"\n",
    "    Generate model response for image-question pair\n",
    "    \"\"\"\n",
    "    prompt = f\"USER: <image>\\n{question}\\nASSISTANT:\"\n",
    "    \n",
    "    inputs = processor(prompt, image, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            do_sample=True,\n",
    "            temperature=0.1,\n",
    "            pad_token_id=processor.tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode and clean response\n",
    "    response = processor.decode(output[0], skip_special_tokens=True)\n",
    "    # Extract only the assistant's response\n",
    "    response = response.split(\"ASSISTANT:\")[-1].strip()\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load BLINK Dataset Examples\n",
    "\n",
    "These examples are from the actual BLINK dataset, including real failure cases identified in LLaVA responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download example images and configuration\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# GitHub raw URL base\n",
    "base_url = \"https://raw.githubusercontent.com/iiscleap/mllm-uncertainty-estimation/main/examples/\"\n",
    "\n",
    "# Create local directory\n",
    "os.makedirs(\"examples\", exist_ok=True)\n",
    "\n",
    "# Download configuration\n",
    "config_url = f\"{base_url}demo_examples.json\"\n",
    "urllib.request.urlretrieve(config_url, \"examples/demo_examples.json\")\n",
    "\n",
    "# Load example configuration\n",
    "with open(\"examples/demo_examples.json\", \"r\") as f:\n",
    "    examples = json.load(f)\n",
    "\n",
    "print(\"Loaded BLINK dataset examples:\")\n",
    "print(f\"- Equivalent examples: {len(examples['equivalent_examples'])}\")\n",
    "print(f\"- Complementary examples: {len(examples['complementary_examples'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download example images\n",
    "image_files = [\n",
    "    \"val_Spatial_Relation_1.jpg\",\n",
    "    \"val_Spatial_Relation_1_contrast1.jpg\", \n",
    "    \"val_Spatial_Relation_1_masking1.jpg\",\n",
    "    \"val_Spatial_Relation_1_negated_contrast1.jpg\"\n",
    "]\n",
    "\n",
    "for img_file in image_files:\n",
    "    img_url = f\"{base_url}{img_file}\"\n",
    "    urllib.request.urlretrieve(img_url, f\"examples/{img_file}\")\n",
    "    print(f\"Downloaded: {img_file}\")\n",
    "\n",
    "print(\"\\nAll images downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FESTA Assessment: Equivalent Input Consistency\n",
    "\n",
    "**Equivalent inputs** have the same semantic meaning but different phrasing. A consistent model should provide similar responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_equivalent_assessment(example_id):\n",
    "    # Find the example\n",
    "    example = next(ex for ex in examples['equivalent_examples'] if ex['id'] == example_id)\n",
    "    \n",
    "    # Load images\n",
    "    orig_img = Image.open(f\"examples/{example['original_image']}\")\n",
    "    pert_img = Image.open(f\"examples/{example['perturbed_image']}\")\n",
    "    \n",
    "    # Generate responses\n",
    "    print(f\"\\nüîç EQUIVALENT INPUT ASSESSMENT: {example['perturbation_type'].upper()}\")\n",
    "    print(f\"Expected failure: {'YES' if example['expected_failure'] else 'NO'}\")\n",
    "    print(f\"Known LLaVA behavior: {example['actual_llava_behavior']}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    orig_response = generate_response(orig_img, example['original_question'])\n",
    "    pert_response = generate_response(pert_img, example['perturbed_question'])\n",
    "    \n",
    "    # Display images side by side\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    axes[0].imshow(orig_img)\n",
    "    axes[0].set_title(\"Original Image\", fontsize=14, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(pert_img)\n",
    "    axes[1].set_title(f\"Perturbed Image ({example['perturbation_type']})\", fontsize=14, fontweight='bold')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Add colored border for failure cases\n",
    "    if example['expected_failure']:\n",
    "        for ax in axes:\n",
    "            rect = patches.Rectangle((0, 0), 1, 1, linewidth=5, edgecolor='red', \n",
    "                                   facecolor='none', transform=ax.transAxes)\n",
    "            ax.add_patch(rect)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display questions and responses\n",
    "    print(f\"\\nüìù ORIGINAL QUESTION: {example['original_question']}\")\n",
    "    print(f\"ü§ñ MODEL RESPONSE: {orig_response}\")\n",
    "    print(f\"\\nüìù PERTURBED QUESTION: {example['perturbed_question']}\")\n",
    "    print(f\"ü§ñ MODEL RESPONSE: {pert_response}\")\n",
    "    \n",
    "    # Consistency assessment\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"üéØ FESTA CONSISTENCY ASSESSMENT:\")\n",
    "    \n",
    "    # Simple consistency check (you can implement more sophisticated methods)\n",
    "    similar = orig_response.lower().strip() == pert_response.lower().strip()\n",
    "    \n",
    "    if similar:\n",
    "        print(\"‚úÖ CONSISTENT: Model provided identical responses\")\n",
    "        consistency_score = 1.0\n",
    "    else:\n",
    "        print(\"‚ùå INCONSISTENT: Model provided different responses\")\n",
    "        consistency_score = 0.0\n",
    "    \n",
    "    print(f\"üìä Consistency Score: {consistency_score:.1f}\")\n",
    "    \n",
    "    if example['expected_failure'] and consistency_score < 1.0:\n",
    "        print(\"üéØ Expected failure confirmed - this perturbation typically causes inconsistencies\")\n",
    "    elif not example['expected_failure'] and consistency_score == 1.0:\n",
    "        print(\"üéØ Expected success confirmed - model maintained consistency\")\n",
    "    \n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return {\n",
    "        'example_id': example_id,\n",
    "        'consistency_score': consistency_score,\n",
    "        'expected_failure': example['expected_failure'],\n",
    "        'original_response': orig_response,\n",
    "        'perturbed_response': pert_response\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run equivalent input assessment - SUCCESS CASE\n",
    "result1 = display_equivalent_assessment(\"blink_val_Spatial_Relation_1_contrast1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run equivalent input assessment - FAILURE CASE  \n",
    "result2 = display_equivalent_assessment(\"blink_val_Spatial_Relation_1_masking1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FESTA Assessment: Complementary Input Consistency\n",
    "\n",
    "**Complementary inputs** have opposite/negated meanings. A consistent model should provide different responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_complementary_assessment(example_id):\n",
    "    # Find the example\n",
    "    example = next(ex for ex in examples['complementary_examples'] if ex['id'] == example_id)\n",
    "    \n",
    "    # Load images\n",
    "    orig_img = Image.open(f\"examples/{example['original_image']}\")\n",
    "    neg_img = Image.open(f\"examples/{example['negated_image']}\")\n",
    "    \n",
    "    # Generate responses\n",
    "    print(f\"\\nüîÑ COMPLEMENTARY INPUT ASSESSMENT: {example['perturbation_type'].upper()}\")\n",
    "    print(f\"Expected failure: {'YES' if example['expected_failure'] else 'NO'}\")\n",
    "    print(f\"Known LLaVA behavior: {example['actual_llava_behavior']}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    orig_response = generate_response(orig_img, example['original_question'])\n",
    "    comp_response = generate_response(neg_img, example['complementary_question'])\n",
    "    \n",
    "    # Display images side by side\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    axes[0].imshow(orig_img)\n",
    "    axes[0].set_title(\"Original Image\", fontsize=14, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(neg_img)\n",
    "    axes[1].set_title(f\"Negated Image ({example['perturbation_type']})\", fontsize=14, fontweight='bold')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Add colored border for failure cases\n",
    "    if example['expected_failure']:\n",
    "        for ax in axes:\n",
    "            rect = patches.Rectangle((0, 0), 1, 1, linewidth=5, edgecolor='orange', \n",
    "                                   facecolor='none', transform=ax.transAxes)\n",
    "            ax.add_patch(rect)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display questions and responses\n",
    "    print(f\"\\nüìù ORIGINAL QUESTION: {example['original_question']}\")\n",
    "    print(f\"ü§ñ MODEL RESPONSE: {orig_response}\")\n",
    "    print(f\"\\nüìù COMPLEMENTARY QUESTION: {example['complementary_question']}\")\n",
    "    print(f\"ü§ñ MODEL RESPONSE: {comp_response}\")\n",
    "    \n",
    "    # Consistency assessment  \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"üéØ FESTA CONSISTENCY ASSESSMENT:\")\n",
    "    \n",
    "    # For complementary inputs, we want DIFFERENT responses\n",
    "    different = orig_response.lower().strip() != comp_response.lower().strip()\n",
    "    \n",
    "    if different:\n",
    "        print(\"‚úÖ CONSISTENT: Model provided different responses for opposite meanings\")\n",
    "        consistency_score = 1.0\n",
    "    else:\n",
    "        print(\"‚ùå INCONSISTENT: Model provided same response for opposite meanings\")\n",
    "        consistency_score = 0.0\n",
    "    \n",
    "    print(f\"üìä Consistency Score: {consistency_score:.1f}\")\n",
    "    \n",
    "    if example['expected_failure'] and consistency_score < 1.0:\n",
    "        print(\"üéØ Expected failure confirmed - model struggles with negated/opposite concepts\")\n",
    "    elif not example['expected_failure'] and consistency_score == 1.0:\n",
    "        print(\"üéØ Expected success confirmed - model correctly handled opposite meanings\")\n",
    "    \n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return {\n",
    "        'example_id': example_id,\n",
    "        'consistency_score': consistency_score,\n",
    "        'expected_failure': example['expected_failure'],\n",
    "        'original_response': orig_response,\n",
    "        'complementary_response': comp_response\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run complementary input assessment - FAILURE CASE\n",
    "result3 = display_complementary_assessment(\"blink_val_Spatial_Relation_1_negated_contrast1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: FESTA Assessment Results\n",
    "\n",
    "The examples above demonstrate real failure cases from the BLINK dataset where LVLMs show inconsistencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all results\n",
    "results = [result1, result2, result3]\n",
    "\n",
    "print(\"üìä FESTA ASSESSMENT SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "total_consistency = sum(r['consistency_score'] for r in results)\n",
    "avg_consistency = total_consistency / len(results)\n",
    "\n",
    "print(f\"Overall Consistency Score: {avg_consistency:.2f} / 1.00\")\n",
    "print(f\"Total Assessments: {len(results)}\")\n",
    "print(f\"Consistent Responses: {int(total_consistency)}\")\n",
    "print(f\"Inconsistent Responses: {len(results) - int(total_consistency)}\")\n",
    "\n",
    "print(\"\\nüìà Breakdown by Assessment Type:\")\n",
    "\n",
    "equiv_results = [r for r in results if 'contrast1' in r['example_id'] or 'masking1' in r['example_id']]\n",
    "comp_results = [r for r in results if 'negated' in r['example_id']]\n",
    "\n",
    "if equiv_results:\n",
    "    equiv_avg = sum(r['consistency_score'] for r in equiv_results) / len(equiv_results)\n",
    "    print(f\"  ‚Ä¢ Equivalent Input Consistency: {equiv_avg:.2f}\")\n",
    "\n",
    "if comp_results:\n",
    "    comp_avg = sum(r['consistency_score'] for r in comp_results) / len(comp_results)\n",
    "    print(f\"  ‚Ä¢ Complementary Input Consistency: {comp_avg:.2f}\")\n",
    "\n",
    "print(\"\\nüéØ Key Findings:\")\n",
    "print(\"  ‚Ä¢ Masking perturbations often cause equivalent input failures\")\n",
    "print(\"  ‚Ä¢ Negated questions frequently lead to complementary input failures\")\n",
    "print(\"  ‚Ä¢ Contrast perturbations tend to be more robust\")\n",
    "\n",
    "print(\"\\nüìö About FESTA:\")\n",
    "print(\"  ‚Ä¢ Framework for systematic LVLM consistency assessment\")\n",
    "print(\"  ‚Ä¢ Identifies failure points using equivalent & complementary inputs\")\n",
    "print(\"  ‚Ä¢ Helps improve model robustness and reliability\")\n",
    "print(\"\\nüîó Learn more: https://github.com/iiscleap/mllm-uncertainty-estimation/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Section: Test Your Own Images\n",
    "\n",
    "Upload your own image and test the model's consistency with equivalent questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import io\n",
    "\n",
    "def test_custom_image():\n",
    "    print(\"Upload an image to test LVLM consistency:\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    if uploaded:\n",
    "        # Get the first uploaded file\n",
    "        filename = list(uploaded.keys())[0]\n",
    "        image = Image.open(io.BytesIO(uploaded[filename]))\n",
    "        \n",
    "        # Display the image\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(image)\n",
    "        plt.title(\"Your Uploaded Image\", fontsize=16, fontweight='bold')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        # Get user questions\n",
    "        print(\"\\nEnter two equivalent questions about this image:\")\n",
    "        q1 = input(\"Question 1: \")\n",
    "        q2 = input(\"Question 2 (equivalent meaning): \")\n",
    "        \n",
    "        # Generate responses\n",
    "        print(\"\\nü§ñ Generating responses...\")\n",
    "        r1 = generate_response(image, q1)\n",
    "        r2 = generate_response(image, q2)\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\nüìù Question 1: {q1}\")\n",
    "        print(f\"ü§ñ Response 1: {r1}\")\n",
    "        print(f\"\\nüìù Question 2: {q2}\")\n",
    "        print(f\"ü§ñ Response 2: {r2}\")\n",
    "        \n",
    "        # Assess consistency\n",
    "        consistent = r1.lower().strip() == r2.lower().strip()\n",
    "        print(f\"\\nüéØ FESTA Assessment: {'CONSISTENT' if consistent else 'INCONSISTENT'}\")\n",
    "        \n",
    "        return {\n",
    "            'questions': [q1, q2],\n",
    "            'responses': [r1, r2],\n",
    "            'consistent': consistent\n",
    "        }\n",
    "    else:\n",
    "        print(\"No image uploaded.\")\n",
    "        return None\n",
    "\n",
    "# Uncomment the line below to test with your own image\n",
    "# custom_result = test_custom_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This demo showed how the **FESTA framework** systematically assesses LVLM consistency using real dataset examples. The framework revealed:\n",
    "\n",
    "- **Equivalent Input Failures**: Masking perturbations often cause inconsistencies\n",
    "- **Complementary Input Failures**: Models struggle with negated/opposite concepts\n",
    "- **Robustness Variations**: Different perturbation types have varying impact\n",
    "\n",
    "FESTA helps identify these failure points to improve LVLM reliability and robustness.\n",
    "\n",
    "**Next Steps**: \n",
    "- Explore the full FESTA framework on GitHub\n",
    "- Try more examples from BLINK and VSR datasets  \n",
    "- Implement consistency improvements based on identified failures\n",
    "\n",
    "üîó **Resources**:\n",
    "- Paper: https://arxiv.org/pdf/2410.23499.pdf\n",
    "- Code: https://github.com/iiscleap/mllm-uncertainty-estimation/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
