{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "festa_unsloth_header"
   },
   "source": [
    "# FESTA Demo - LLaVA 1.6 7B with Unsloth (2x Faster!)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/iiscleap/mllm-uncertainty-estimation/blob/main/festa_demo/FESTA_Unsloth_Demo.ipynb)\n",
    "\n",
    "<div class=\"align-center\">\n",
    "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "</div>\n",
    "\n",
    "**Optimized FESTA notebook using Unsloth for 2-5x faster inference with 70% less memory!**\n",
    "\n",
    "Test LLaVA 1.6 7B model with FESTA example images using research-grade prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "# Install Unsloth for optimized inference\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Optimized Colab installation\n",
    "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "    !pip install pillow requests matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import json\n",
    "import requests\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "import warnings\n",
    "from transformers import logging\n",
    "\n",
    "# Suppress warnings (same as research script)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsloth optimized model loading\n",
    "print(\"Loading Unsloth optimized LLaVA model...\")\n",
    "\n",
    "max_seq_length = 2048  # Choose any! Auto supports RoPE Scaling\n",
    "dtype = None           # None for auto detection. Float16 for Tesla T4\n",
    "load_in_4bit = True    # Use 4bit quantization for memory efficiency\n",
    "\n",
    "# Load the Unsloth optimized LLaVA model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/llava-v1.6-mistral-7b-hf\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "# Enable native 2x faster inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "print(\"‚úÖ Unsloth optimized LLaVA model loaded successfully!\")\n",
    "print(\"üöÄ 2-5x faster inference with 70% less memory usage!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GitHub base URL for examples\n",
    "base_url = \"https://raw.githubusercontent.com/iiscleap/mllm-uncertainty-estimation/main/festa_demo/examples/\"\n",
    "\n",
    "def load_image_from_url(image_name):\n",
    "    \"\"\"Load image from GitHub repository\"\"\"\n",
    "    url = base_url + image_name\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content))\n",
    "\n",
    "def create_llava_prompt(question, dataset_type=\"blink\"):\n",
    "    \"\"\"Create LLaVA prompt with exact same format as research script\"\"\"\n",
    "    \n",
    "    # Set choices based on dataset (same as research script)\n",
    "    if dataset_type == \"blink\":\n",
    "        choices = \"A. Yes\\nB. No\"\n",
    "    elif dataset_type == \"vsr\":\n",
    "        choices = \"A. True\\nB. False\"\n",
    "    else:\n",
    "        choices = \"A. Yes\\nB. No\"\n",
    "    \n",
    "    # Create instruction with exact same format as research script\n",
    "    instruction = f\"{question}\\nChoices:\\n{choices}\\nReturn only the option (A or B), and nothing else.\\nMAKE SURE your output is A or B\"\n",
    "    \n",
    "    # LLaVA conversation format\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": instruction},\n",
    "                {\"type\": \"image\"},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    return conversation\n",
    "\n",
    "def generate_response_unsloth(image, question, dataset_type=\"blink\"):\n",
    "    \"\"\"Generate response using Unsloth optimized LLaVA with research-grade prompting\"\"\"\n",
    "    \n",
    "    conversation = create_llava_prompt(question, dataset_type)\n",
    "    \n",
    "    # Apply chat template (same as research script)\n",
    "    prompt = tokenizer.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "    inputs = tokenizer(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate with Unsloth optimizations (same parameters as research script)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1,  # Single token A/B response\n",
    "            use_cache=True,    # Unsloth optimization\n",
    "            do_sample=False,   # Deterministic\n",
    "            temperature=1.0,\n",
    "        )\n",
    "    \n",
    "    # Extract response (same as research script)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()[-1]\n",
    "    \n",
    "    return response\n",
    "\n",
    "def test_example_unsloth(image_name, question, title, dataset_type=\"blink\"):\n",
    "    \"\"\"Test example with Unsloth optimized inference\"\"\"\n",
    "    image = load_image_from_url(image_name)\n",
    "    \n",
    "    # Measure inference time\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    response = generate_response_unsloth(image, question, dataset_type)\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    # Determine full answer text\n",
    "    if dataset_type == \"blink\":\n",
    "        full_answer = \"A (Yes)\" if response == \"A\" else \"B (No)\" if response == \"B\" else response\n",
    "    elif dataset_type == \"vsr\":\n",
    "        full_answer = \"A (True)\" if response == \"A\" else \"B (False)\" if response == \"B\" else response\n",
    "    else:\n",
    "        full_answer = \"A (Yes)\" if response == \"A\" else \"B (No)\" if response == \"B\" else response\n",
    "    \n",
    "    # Display results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"{title}\\nQuestion: {question}\\nLLaVA Answer: {full_answer}\\n‚ö° Inference Time: {inference_time:.2f}s\", \n",
    "              fontsize=12, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"üöÄ Unsloth optimized inference: {inference_time:.2f}s\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"‚úÖ Unsloth optimized functions loaded with research-grade prompting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Test with 6 FESTA Examples (Unsloth Optimized)\n",
    "\n",
    "Using **Unsloth optimizations** with the exact same prompting setup as the FESTA research paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Original Spatial Relation\n",
    "print(\"üîç Testing Example 1 with Unsloth optimizations...\")\n",
    "response = test_example_unsloth(\n",
    "    \"val_Spatial_Relation_1.jpg\",\n",
    "    \"Is the car beneath the cat?\",\n",
    "    \"Example 1: Original Spatial Relation (Unsloth Optimized)\",\n",
    "    \"blink\"\n",
    ")\n",
    "print(f\"Raw Response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Contrast Perturbation (Equivalent Sample)\n",
    "print(\"üîç Testing Example 2 with Unsloth optimizations...\")\n",
    "response = test_example_unsloth(\n",
    "    \"val_Spatial_Relation_1_contrast1.jpg\",\n",
    "    \"Is the car beneath the cat?\",\n",
    "    \"Example 2: Contrast Perturbation (Should be same as Example 1)\",\n",
    "    \"blink\"\n",
    ")\n",
    "print(f\"Raw Response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Masking Perturbation (Equivalent Sample)\n",
    "print(\"üîç Testing Example 3 with Unsloth optimizations...\")\n",
    "response = test_example_unsloth(\n",
    "    \"val_Spatial_Relation_1_masking1.jpg\",\n",
    "    \"Is the car beneath the cat?\",\n",
    "    \"Example 3: Masking Perturbation (Should be same as Example 1)\",\n",
    "    \"blink\"\n",
    ")\n",
    "print(f\"Raw Response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Negated/Complementary Version (Should toggle answer)\n",
    "print(\"üîç Testing Example 4 with Unsloth optimizations...\")\n",
    "response = test_example_unsloth(\n",
    "    \"val_Spatial_Relation_1_negated_contrast1.jpg\",\n",
    "    \"Is the car beneath the cat?\",\n",
    "    \"Example 4: Negated Scene (Should give opposite answer)\",\n",
    "    \"blink\"\n",
    ")\n",
    "print(f\"Raw Response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5: Different Scene Original\n",
    "print(\"üîç Testing Example 5 with Unsloth optimizations...\")\n",
    "response = test_example_unsloth(\n",
    "    \"val_Spatial_Relation_5.jpg\",\n",
    "    \"Are there animals in this image?\",\n",
    "    \"Example 5: Different Scene Original\",\n",
    "    \"blink\"\n",
    ")\n",
    "print(f\"Raw Response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6: Different Scene Blur (Equivalent Sample)\n",
    "print(\"üîç Testing Example 6 with Unsloth optimizations...\")\n",
    "response = test_example_unsloth(\n",
    "    \"val_Spatial_Relation_5_blur1.jpg\",\n",
    "    \"Are there animals in this image?\",\n",
    "    \"Example 6: Blur Perturbation (Should be same as Example 5)\",\n",
    "    \"blink\"\n",
    ")\n",
    "print(f\"Raw Response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° FESTA Consistency Test (Unsloth Optimized)\n",
    "\n",
    "Test equivalent sampling with optimized performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FESTA Equivalent Sampling Test with performance tracking\n",
    "print(\"üîç FESTA Equivalent Sampling Test (Unsloth Optimized):\")\n",
    "print(\"Testing same question on original vs perturbed images (should be consistent)\\n\")\n",
    "\n",
    "import time\n",
    "question = \"Is the car beneath the cat?\"\n",
    "images = [\n",
    "    \"val_Spatial_Relation_1.jpg\",\n",
    "    \"val_Spatial_Relation_1_contrast1.jpg\", \n",
    "    \"val_Spatial_Relation_1_masking1.jpg\"\n",
    "]\n",
    "\n",
    "responses = []\n",
    "total_time = 0\n",
    "\n",
    "for i, img in enumerate(images):\n",
    "    start_time = time.time()\n",
    "    image = load_image_from_url(img)\n",
    "    resp = generate_response_unsloth(image, question, \"blink\")\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    responses.append(resp)\n",
    "    total_time += inference_time\n",
    "    \n",
    "    img_type = [\"Original\", \"Contrast\", \"Masking\"][i]\n",
    "    print(f\"{img_type:>10}: {resp} (‚ö° {inference_time:.2f}s)\")\n",
    "\n",
    "# Check consistency\n",
    "all_same = len(set(responses)) == 1\n",
    "print(f\"\\n{'‚úÖ CONSISTENT' if all_same else '‚ùå INCONSISTENT'}: {'All responses match' if all_same else 'Responses vary across equivalent samples'}\")\n",
    "print(f\"üöÄ Total inference time with Unsloth: {total_time:.2f}s\")\n",
    "print(f\"‚ö° Average time per sample: {total_time/3:.2f}s\")\n",
    "\n",
    "if not all_same:\n",
    "    print(\"‚ö†Ô∏è  FESTA Equivalent Sampling Failure Detected!\")\n",
    "else:\n",
    "    print(\"üéâ Model shows consistency across equivalent perturbations!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark: Compare speeds (Optional)\n",
    "def benchmark_inference():\n",
    "    \"\"\"Quick benchmark of Unsloth optimized inference\"\"\"\n",
    "    print(\"üèÅ Benchmarking Unsloth optimized inference...\")\n",
    "    \n",
    "    image = load_image_from_url(\"val_Spatial_Relation_1.jpg\")\n",
    "    question = \"Is the car beneath the cat?\"\n",
    "    \n",
    "    # Run 5 inference samples\n",
    "    times = []\n",
    "    for i in range(5):\n",
    "        start_time = time.time()\n",
    "        response = generate_response_unsloth(image, question, \"blink\")\n",
    "        inference_time = time.time() - start_time\n",
    "        times.append(inference_time)\n",
    "        print(f\"Run {i+1}: {response} (‚ö° {inference_time:.2f}s)\")\n",
    "    \n",
    "    avg_time = sum(times) / len(times)\n",
    "    print(f\"\\nüöÄ Average Unsloth inference time: {avg_time:.2f}s\")\n",
    "    print(f\"‚ö° 2-5x faster than standard transformers!\")\n",
    "\n",
    "# Uncomment to run benchmark\n",
    "# benchmark_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom testing function with Unsloth optimizations\n",
    "def test_custom_unsloth():\n",
    "    \"\"\"Custom testing with Unsloth optimized inference\"\"\"\n",
    "    # Change these values to test other images/questions\n",
    "    image_name = \"val_Spatial_Relation_1.jpg\"  # Change this\n",
    "    question = \"Is the car beneath the cat?\"      # Change this\n",
    "    dataset_type = \"blink\"                       # \"blink\" or \"vsr\"\n",
    "    \n",
    "    print(\"üîç Custom test with Unsloth optimizations...\")\n",
    "    response = test_example_unsloth(image_name, question, \"Custom Test (Unsloth)\", dataset_type)\n",
    "    print(f\"Custom Response: {response}\")\n",
    "\n",
    "# Uncomment to use custom testing\n",
    "# test_custom_unsloth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Performance & Research Notes\n",
    "\n",
    "### üöÄ **Unsloth Optimizations:**\n",
    "- **2-5x faster inference** compared to standard transformers\n",
    "- **70% less memory usage** with 4-bit quantization\n",
    "- **Native optimizations** for Google Colab T4 GPUs\n",
    "- **Same model quality** as original LLaVA 1.6 7B\n",
    "\n",
    "### üî¨ **Research Fidelity:**\n",
    "This notebook uses the **exact same prompting setup** as the FESTA research paper:\n",
    "\n",
    "- **System Prompt**: `\"{question}\\nChoices:\\n{choices}\\nReturn only the option (A or B), and nothing else.\\nMAKE SURE your output is A or B\"`\n",
    "- **Chat Template**: Applied via `tokenizer.apply_chat_template()`\n",
    "- **Generation**: `max_new_tokens=1` for single token A/B response\n",
    "- **Model**: `unsloth/llava-v1.6-mistral-7b-hf` with optimizations\n",
    "\n",
    "### üéØ **FESTA Framework Tests:**\n",
    "- **Equivalent Samples**: Same question, different perturbations ‚Üí Should give consistent answers\n",
    "- **Complementary Samples**: Opposite scenarios ‚Üí Should give different answers\n",
    "\n",
    "### üîó **Links:**\n",
    "- **Unsloth**: https://unsloth.ai/\n",
    "- **Model**: https://huggingface.co/unsloth/llava-v1.6-mistral-7b-hf\n",
    "- **Discord**: https://discord.gg/unsloth\n",
    "\n",
    "---\n",
    "*Optimized with ‚ù§Ô∏è by Unsloth AI for 2-5x faster inference!*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
