{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "festa_simple_header"
   },
   "source": [
    "# FESTA Demo - LLaVA 1.6 7B Testing\n",
    "\n",
    "Simple notebook to test LLaVA 1.6 7B model with FESTA example images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision transformers pillow accelerate bitsandbytes\n",
    "!pip install git+https://github.com/LLaVA-VL/LLaVA-NeXT.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import requests\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "from io import BytesIO\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LLaVA 1.6 7B model\n",
    "model_id = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "\n",
    "processor = LlavaNextProcessor.from_pretrained(model_id)\n",
    "model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.float16, \n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"LLaVA 1.6 7B model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GitHub base URL for examples\n",
    "base_url = \"https://raw.githubusercontent.com/iiscleap/mllm-uncertainty-estimation/main/examples/\"\n",
    "\n",
    "def load_image_from_url(image_name):\n",
    "    url = base_url + image_name\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content))\n",
    "\n",
    "def generate_response(image, question):\n",
    "    prompt = f\"USER: <image>\\n{question}\\nASSISTANT:\"\n",
    "    \n",
    "    inputs = processor(prompt, image, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=False,\n",
    "            temperature=0.7\n",
    "        )\n",
    "    \n",
    "    response = processor.decode(output[0], skip_special_tokens=True)\n",
    "    response = response.split(\"ASSISTANT:\")[-1].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "def test_example(image_name, question, title):\n",
    "    image = load_image_from_url(image_name)\n",
    "    response = generate_response(image, question)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"{title}\\nQuestion: {question}\\nAnswer: {response}\", fontsize=12, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with 6 FESTA Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Original Spatial Relation\n",
    "response = test_example(\n",
    "    \"val_Spatial_Relation_1.jpg\",\n",
    "    \"Is the car beneath the cat?\",\n",
    "    \"Example 1: Original Spatial Relation\"\n",
    ")\n",
    "print(f\"Response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Contrast Perturbation\n",
    "response = test_example(\n",
    "    \"val_Spatial_Relation_1_contrast1.jpg\",\n",
    "    \"Is the car beneath the cat?\",\n",
    "    \"Example 2: Contrast Perturbation\"\n",
    ")\n",
    "print(f\"Response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Masking Perturbation\n",
    "response = test_example(\n",
    "    \"val_Spatial_Relation_1_masking1.jpg\",\n",
    "    \"Is the car beneath the cat?\",\n",
    "    \"Example 3: Masking Perturbation\"\n",
    ")\n",
    "print(f\"Response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Negated/Complementary Version\n",
    "response = test_example(\n",
    "    \"val_Spatial_Relation_1_negated_contrast1.jpg\",\n",
    "    \"Is the car beneath the cat?\",\n",
    "    \"Example 4: Negated/Complementary Version\"\n",
    ")\n",
    "print(f\"Response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5: Different Scene Original\n",
    "response = test_example(\n",
    "    \"val_Spatial_Relation_5.jpg\",\n",
    "    \"Are there animals in this image?\",\n",
    "    \"Example 5: Different Scene Original\"\n",
    ")\n",
    "print(f\"Response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6: Different Scene Blur\n",
    "response = test_example(\n",
    "    \"val_Spatial_Relation_5_blur1.jpg\",\n",
    "    \"Are there animals in this image?\",\n",
    "    \"Example 6: Different Scene Blur\"\n",
    ")\n",
    "print(f\"Response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom testing function - modify as needed\n",
    "def test_custom():\n",
    "    # Change these values to test other images/questions\n",
    "    image_name = \"val_Spatial_Relation_1.jpg\"  # Change this\n",
    "    question = \"What do you see in this image?\"  # Change this\n",
    "    \n",
    "    response = test_example(image_name, question, \"Custom Test\")\n",
    "    print(f\"Custom Response: {response}\")\n",
    "\n",
    "# Uncomment to use custom testing\n",
    "# test_custom()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
