{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "festa_simple_header"
   },
   "source": [
    "# FESTA Demo - LLaVA 1.6 7B Testing\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/iiscleap/mllm-uncertainty-estimation/blob/main/festa_demo/FESTA_Simple_Demo.ipynb)\n",
    "\n",
    "Simple notebook to test LLaVA 1.6 7B model with FESTA example images using the exact same prompting as the research setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision transformers pillow accelerate bitsandbytes\n",
    "!pip install git+https://github.com/LLaVA-VL/LLaVA-NeXT.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import requests\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "from io import BytesIO\n",
    "import warnings\n",
    "from transformers import logging\n",
    "\n",
    "# Suppress warnings and transformers logging (same as research script)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LLaVA 1.6 7B model (exactly same as research script)\n",
    "print(\"Loading Llava model...\")\n",
    "processor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n",
    "model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "    \"llava-hf/llava-v1.6-mistral-7b-hf\", \n",
    "    torch_dtype=torch.float16, \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GitHub base URL for examples\n",
    "base_url = \"https://raw.githubusercontent.com/iiscleap/mllm-uncertainty-estimation/main/festa_demo/examples/\"\n",
    "\n",
    "def load_image_from_url(image_name):\n",
    "    url = base_url + image_name\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content))\n",
    "\n",
    "def generate_response(image, question, dataset_type=\"blink\"):\n",
    "    \"\"\"Generate response using exact same prompt format as research script\"\"\"\n",
    "    \n",
    "    # Set choices based on dataset (same as research script)\n",
    "    if dataset_type == \"blink\":\n",
    "        choices = \"A. Yes\\nB. No\"\n",
    "    elif dataset_type == \"vsr\":\n",
    "        choices = \"A. True\\nB. False\"\n",
    "    else:\n",
    "        choices = \"A. Yes\\nB. No\"  # default to blink format\n",
    "    \n",
    "    # Create instruction with exact same format as research script\n",
    "    instruction = f\"{question}\\nChoices:\\n{choices}\\nReturn only the option (A or B), and nothing else.\\nMAKE SURE your output is A or B\"\n",
    "    \n",
    "    # Create conversation with exact same structure as research script\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": instruction},\n",
    "                {\"type\": \"image\"},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template and generate (same as research script)\n",
    "    prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate with exact same parameters as research script\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_new_tokens=1)\n",
    "        res = processor.decode(output[0], skip_special_tokens=True).strip()[-1]\n",
    "    \n",
    "    return res\n",
    "\n",
    "def test_example(image_name, question, title, dataset_type=\"blink\"):\n",
    "    \"\"\"Test example with research-grade prompting\"\"\"\n",
    "    image = load_image_from_url(image_name)\n",
    "    response = generate_response(image, question, dataset_type)\n",
    "    \n",
    "    # Determine full answer text\n",
    "    if dataset_type == \"blink\":\n",
    "        full_answer = \"A (Yes)\" if response == \"A\" else \"B (No)\" if response == \"B\" else response\n",
    "    elif dataset_type == \"vsr\":\n",
    "        full_answer = \"A (True)\" if response == \"A\" else \"B (False)\" if response == \"B\" else response\n",
    "    else:\n",
    "        full_answer = \"A (Yes)\" if response == \"A\" else \"B (No)\" if response == \"B\" else response\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"{title}\\nQuestion: {question}\\nLLaVA Answer: {full_answer}\", fontsize=12, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"‚úÖ Functions loaded with research-grade prompting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with 6 FESTA Examples\n",
    "\n",
    "Using the exact same prompting setup as the FESTA research paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Original Spatial Relation\n",
    "response = test_example(\n",
    "    \"val_Spatial_Relation_1.jpg\",\n",
    "    \"Is the car beneath the cat?\",\n",
    "    \"Example 1: Original Spatial Relation\",\n",
    "    \"blink\"\n",
    ")\n",
    "print(f\"Raw Response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Contrast Perturbation (Equivalent Sample)\n",
    "response = test_example(\n",
    "    \"val_Spatial_Relation_1_contrast1.jpg\",\n",
    "    \"Is the car beneath the cat?\",\n",
    "    \"Example 2: Contrast Perturbation (Should be same as Example 1)\",\n",
    "    \"blink\"\n",
    ")\n",
    "print(f\"Raw Response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Masking Perturbation (Equivalent Sample)\n",
    "response = test_example(\n",
    "    \"val_Spatial_Relation_1_masking1.jpg\",\n",
    "    \"Is the car beneath the cat?\",\n",
    "    \"Example 3: Masking Perturbation (Should be same as Example 1)\",\n",
    "    \"blink\"\n",
    ")\n",
    "print(f\"Raw Response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Negated/Complementary Version (Should toggle answer)\n",
    "response = test_example(\n",
    "    \"val_Spatial_Relation_1_negated_contrast1.jpg\",\n",
    "    \"Is the car beneath the cat?\",\n",
    "    \"Example 4: Negated Scene (Should give opposite answer)\",\n",
    "    \"blink\"\n",
    ")\n",
    "print(f\"Raw Response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5: Different Scene Original\n",
    "response = test_example(\n",
    "    \"val_Spatial_Relation_5.jpg\",\n",
    "    \"Are there animals in this image?\",\n",
    "    \"Example 5: Different Scene Original\",\n",
    "    \"blink\"\n",
    ")\n",
    "print(f\"Raw Response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6: Different Scene Blur (Equivalent Sample)\n",
    "response = test_example(\n",
    "    \"val_Spatial_Relation_5_blur1.jpg\",\n",
    "    \"Are there animals in this image?\",\n",
    "    \"Example 6: Blur Perturbation (Should be same as Example 5)\",\n",
    "    \"blink\"\n",
    ")\n",
    "print(f\"Raw Response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Responses\n",
    "\n",
    "Check for FESTA failure patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multiple times to see consistency (FESTA equivalent sampling)\n",
    "print(\"üîç FESTA Equivalent Sampling Test:\")\n",
    "print(\"Testing same question on original vs perturbed images (should be consistent)\\n\")\n",
    "\n",
    "question = \"Is the car beneath the cat?\"\n",
    "images = [\n",
    "    \"val_Spatial_Relation_1.jpg\",\n",
    "    \"val_Spatial_Relation_1_contrast1.jpg\", \n",
    "    \"val_Spatial_Relation_1_masking1.jpg\"\n",
    "]\n",
    "\n",
    "responses = []\n",
    "for i, img in enumerate(images):\n",
    "    image = load_image_from_url(img)\n",
    "    resp = generate_response(image, question, \"blink\")\n",
    "    responses.append(resp)\n",
    "    img_type = [\"Original\", \"Contrast\", \"Masking\"][i]\n",
    "    print(f\"{img_type:>10}: {resp}\")\n",
    "\n",
    "# Check consistency\n",
    "all_same = len(set(responses)) == 1\n",
    "print(f\"\\n{'‚úÖ CONSISTENT' if all_same else '‚ùå INCONSISTENT'}: {'All responses match' if all_same else 'Responses vary across equivalent samples'}\")\n",
    "\n",
    "if not all_same:\n",
    "    print(\"‚ö†Ô∏è  FESTA Equivalent Sampling Failure Detected!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom testing function - modify as needed\n",
    "def test_custom():\n",
    "    # Change these values to test other images/questions\n",
    "    image_name = \"val_Spatial_Relation_1.jpg\"  # Change this\n",
    "    question = \"Is the car beneath the cat?\"      # Change this\n",
    "    dataset_type = \"blink\"                       # \"blink\" or \"vsr\"\n",
    "    \n",
    "    response = test_example(image_name, question, \"Custom Test\", dataset_type)\n",
    "    print(f\"Custom Response: {response}\")\n",
    "\n",
    "# Uncomment to use custom testing\n",
    "# test_custom()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Notes\n",
    "\n",
    "This notebook uses the **exact same prompting setup** as the FESTA research paper:\n",
    "\n",
    "- **System Prompt**: `\"{question}\\nChoices:\\n{choices}\\nReturn only the option (A or B), and nothing else.\\nMAKE SURE your output is A or B\"`\n",
    "- **Chat Template**: Applied via `processor.apply_chat_template()`\n",
    "- **Generation**: `max_new_tokens=1` for single token A/B response\n",
    "- **Model**: `llava-hf/llava-v1.6-mistral-7b-hf` with `torch.float16`\n",
    "\n",
    "**FESTA Framework Tests:**\n",
    "- **Equivalent Samples**: Same question, different perturbations ‚Üí Should give consistent answers\n",
    "- **Complementary Samples**: Opposite scenarios ‚Üí Should give different answers"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
