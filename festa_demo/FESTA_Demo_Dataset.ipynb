{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üé≠ FESTA Framework Demo\n",
        "\n",
        "Demo for **FESTA (Framework for Evaluating Semantic and Temporal Assumptions)** using BLINK and VSR datasets.\n",
        "\n",
        "**Repository**: [https://github.com/iiscleap/mllm-uncertainty-estimation](https://github.com/iiscleap/mllm-uncertainty-estimation)"
      ],
      "metadata": {
        "id": "demo_intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install packages\n",
        "!pip install -q transformers==4.36.2 torch torchvision pillow accelerate requests matplotlib seaborn numpy pandas"
      ],
      "metadata": {
        "id": "install_packages"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dataset examples\n",
        "import os, requests, json\n",
        "os.makedirs('examples', exist_ok=True)\n",
        "\n",
        "base_url = \"https://raw.githubusercontent.com/iiscleap/mllm-uncertainty-estimation/main/examples/\"\n",
        "example_images = [\n",
        "    \"val_Spatial_Relation_1.jpg\", \"val_Spatial_Relation_1_blur1.jpg\",\n",
        "    \"val_Spatial_Relation_10.jpg\", \"val_Spatial_Relation_10_contrast1.jpg\",\n",
        "    \"val_Spatial_Relation_25.jpg\", \"val_Spatial_Relation_25_noise1.jpg\",\n",
        "    \"val_Spatial_Relation_50.jpg\", \"val_Spatial_Relation_50_bw1.jpg\",\n",
        "    \"val_Spatial_Relation_75.jpg\", \"val_Spatial_Relation_75_masking1.jpg\",\n",
        "    \"val_Spatial_Reasoning_111.jpg\", \"val_Spatial_Reasoning_125.jpg\"\n",
        "]\n",
        "\n",
        "print(\"üì• Downloading examples...\")\n",
        "for img_name in example_images:\n",
        "    try:\n",
        "        response = requests.get(base_url + img_name)\n",
        "        if response.status_code == 200:\n",
        "            with open(f'examples/{img_name}', 'wb') as f:\n",
        "                f.write(response.content)\n",
        "            print(f\"‚úÖ {img_name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå {img_name}: {e}\")\n",
        "\n",
        "print(\"‚úÖ Ready!\")"
      ],
      "metadata": {
        "id": "download_examples"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, matplotlib.pyplot as plt, seaborn as sns\n",
        "from PIL import Image\n",
        "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(f\"üî• CUDA: {torch.cuda.is_available()}\")\n",
        "\n",
        "# Load demo config with real CSV data\n",
        "demo_config = {\n",
        "    \"equivalent_examples\": [\n",
        "        {\n",
        "            \"id\": \"blink_val_Spatial_Relation_1_blur1\",\n",
        "            \"dataset\": \"BLINK\",\n",
        "            \"original_image\": \"val_Spatial_Relation_1.jpg\",\n",
        "            \"perturbed_image\": \"val_Spatial_Relation_1_blur1.jpg\",\n",
        "            \"original_question\": \"Is the cat standing above a car?\",\n",
        "            \"perturbed_question\": \"How are the cat and the car situated relative to each other?\",\n",
        "            \"expected_answer\": \"B - The cat is not above the car\",\n",
        "            \"expected_failure\": False,\n",
        "            \"perturbation_type\": \"blur1\"\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"blink_val_Spatial_Relation_10_contrast1\",\n",
        "            \"dataset\": \"BLINK\",\n",
        "            \"original_image\": \"val_Spatial_Relation_10.jpg\",\n",
        "            \"perturbed_image\": \"val_Spatial_Relation_10_contrast1.jpg\",\n",
        "            \"original_question\": \"Is the person in contact with the laptop?\",\n",
        "            \"perturbed_question\": \"What's the relative placement of the person and the laptop?\",\n",
        "            \"expected_answer\": \"A - The person is in contact with the laptop\",\n",
        "            \"expected_failure\": False,\n",
        "            \"perturbation_type\": \"contrast1\"\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"blink_val_Spatial_Relation_25_noise1\",\n",
        "            \"dataset\": \"BLINK\",\n",
        "            \"original_image\": \"val_Spatial_Relation_25.jpg\",\n",
        "            \"perturbed_image\": \"val_Spatial_Relation_25_noise1.jpg\",\n",
        "            \"original_question\": \"Is the sheep situated outside the bottle?\",\n",
        "            \"perturbed_question\": \"How are the sheep and the bottle situated relative to each other?\",\n",
        "            \"expected_answer\": \"A - The sheep is outside the bottle\",\n",
        "            \"expected_failure\": False,\n",
        "            \"perturbation_type\": \"noise1\"\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"blink_val_Spatial_Relation_50_bw1\",\n",
        "            \"dataset\": \"BLINK\",\n",
        "            \"original_image\": \"val_Spatial_Relation_50.jpg\",\n",
        "            \"perturbed_image\": \"val_Spatial_Relation_50_bw1.jpg\",\n",
        "            \"original_question\": \"Is the sandwich positioned to the left of the laptop?\",\n",
        "            \"perturbed_question\": \"What's the relative placement of the sandwich and the laptop?\",\n",
        "            \"expected_answer\": \"B - The sandwich is not to the left of the laptop\",\n",
        "            \"expected_failure\": False,\n",
        "            \"perturbation_type\": \"bw1\"\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"blink_val_Spatial_Relation_75_masking1\",\n",
        "            \"dataset\": \"BLINK\",\n",
        "            \"original_image\": \"val_Spatial_Relation_75.jpg\",\n",
        "            \"perturbed_image\": \"val_Spatial_Relation_75_masking1.jpg\",\n",
        "            \"original_question\": \"Is there a person in physical contact with the zebra?\",\n",
        "            \"perturbed_question\": \"What is the spatial arrangement of the person and the zebra?\",\n",
        "            \"expected_answer\": \"B - The person is not in physical contact with the zebra\",\n",
        "            \"expected_failure\": True,\n",
        "            \"perturbation_type\": \"masking1\"\n",
        "        }\n",
        "    ],\n",
        "    \"complementary_examples\": [\n",
        "        {\n",
        "            \"id\": \"blink_val_Spatial_Relation_1_negated\",\n",
        "            \"dataset\": \"BLINK\",\n",
        "            \"image\": \"val_Spatial_Relation_1.jpg\",\n",
        "            \"original_question\": \"Is the cat standing above a car?\",\n",
        "            \"complementary_question\": \"Is the car positioned ahead of the cat?\",\n",
        "            \"expected_original_answer\": \"B - The cat is not above the car\",\n",
        "            \"expected_complementary_answer\": \"B - The car is not ahead of the cat\",\n",
        "            \"expected_failure\": False\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"blink_val_Spatial_Relation_10_negated\",\n",
        "            \"dataset\": \"BLINK\",\n",
        "            \"image\": \"val_Spatial_Relation_10.jpg\",\n",
        "            \"original_question\": \"Is the person in contact with the laptop?\",\n",
        "            \"complementary_question\": \"Is the laptop located at a distance from the individual?\",\n",
        "            \"expected_original_answer\": \"A - The person is in contact with the laptop\",\n",
        "            \"expected_complementary_answer\": \"A - The laptop is at a distance from the individual\",\n",
        "            \"expected_failure\": False\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"vsr_val_Spatial_Reasoning_111\",\n",
        "            \"dataset\": \"VSR\",\n",
        "            \"image\": \"val_Spatial_Reasoning_111.jpg\",\n",
        "            \"original_question\": \"The dog has the motorcycle positioned behind it.\",\n",
        "            \"complementary_question\": \"The dog is behind the motorcycle.\",\n",
        "            \"expected_original_answer\": \"A - The dog is behind the motorcycle\",\n",
        "            \"expected_complementary_answer\": \"A - The dog is behind the motorcycle\",\n",
        "            \"expected_failure\": False\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"vsr_val_Spatial_Reasoning_125\",\n",
        "            \"dataset\": \"VSR\",\n",
        "            \"image\": \"val_Spatial_Reasoning_125.jpg\",\n",
        "            \"original_question\": \"The horse is positioned above the chair.\",\n",
        "            \"complementary_question\": \"The horse has the chair positioned behind it.\",\n",
        "            \"expected_original_answer\": \"B - The horse is not above the chair\",\n",
        "            \"expected_complementary_answer\": \"B - The horse is not behind the chair\",\n",
        "            \"expected_failure\": True\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"üìã Config loaded!\")"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load LLaVA model\n",
        "print(\"üì• Loading model...\")\n",
        "\n",
        "model_name = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
        "processor = LlavaNextProcessor.from_pretrained(model_name)\n",
        "model = LlavaNextForConditionalGeneration.from_pretrained(\n",
        "    model_name, torch_dtype=torch.float16, device_map=\"auto\", load_in_8bit=True\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Model loaded!\")"
      ],
      "metadata": {
        "id": "load_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper functions\n",
        "def get_model_response(image, question, max_new_tokens=50):\n",
        "    prompt = f\"USER: <image>\\n{question}\\nASSISTANT:\"\n",
        "    inputs = processor(prompt, image, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            **inputs, max_new_tokens=max_new_tokens, do_sample=False,\n",
        "            pad_token_id=processor.tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    response = processor.decode(output[0], skip_special_tokens=True)\n",
        "    return response.split(\"ASSISTANT:\")[-1].strip()\n",
        "\n",
        "def load_example_image(image_name):\n",
        "    try:\n",
        "        return Image.open(f'examples/{image_name}')\n",
        "    except:\n",
        "        print(f\"‚ö†Ô∏è Could not load {image_name}\")\n",
        "        return Image.new('RGB', (224, 224), color='lightgray')\n",
        "\n",
        "def display_example(orig_img, pert_img, orig_q, pert_q, orig_resp, pert_resp, is_failure, desc):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "    \n",
        "    axes[0].imshow(orig_img)\n",
        "    axes[0].set_title(\"Original\", fontweight='bold')\n",
        "    axes[0].axis('off')\n",
        "    \n",
        "    axes[1].imshow(pert_img)\n",
        "    axes[1].set_title(\"Perturbed\", fontweight='bold')\n",
        "    axes[1].axis('off')\n",
        "    \n",
        "    status = \"‚ùå FAILURE\" if is_failure else \"‚úÖ SUCCESS\"\n",
        "    color = \"red\" if is_failure else \"green\"\n",
        "    plt.suptitle(f\"EQUIVALENT - {status}\", fontsize=14, fontweight='bold', color=color)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"üìù Original Q: {orig_q}\")\n",
        "    print(f\"ü§ñ Original R: {orig_resp}\")\n",
        "    print(f\"üìù Perturbed Q: {pert_q}\")\n",
        "    print(f\"ü§ñ Perturbed R: {pert_resp}\\n\")\n",
        "\n",
        "def display_complementary_example(img, orig_q, comp_q, orig_resp, comp_resp, is_failure, desc):\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
        "    ax.imshow(img)\n",
        "    ax.axis('off')\n",
        "    \n",
        "    status = \"‚ùå FAILURE\" if is_failure else \"‚úÖ SUCCESS\"\n",
        "    color = \"red\" if is_failure else \"green\"\n",
        "    plt.suptitle(f\"COMPLEMENTARY - {status}\", fontsize=14, fontweight='bold', color=color)\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"üìù Original Q: {orig_q}\")\n",
        "    print(f\"ü§ñ Original R: {orig_resp}\")\n",
        "    print(f\"üìù Complementary Q: {comp_q}\")\n",
        "    print(f\"ü§ñ Complementary R: {comp_resp}\\n\")\n",
        "\n",
        "print(\"üõ†Ô∏è Functions ready!\")"
      ],
      "metadata": {
        "id": "helper_functions"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run equivalent examples\n",
        "equivalent_results = []\n",
        "\n",
        "for i, example in enumerate(demo_config[\"equivalent_examples\"], 1):\n",
        "    print(f\"üß™ EQUIVALENT EXAMPLE {i}: {example['dataset']} - {example['perturbation_type']}\")\n",
        "    \n",
        "    orig_img = load_example_image(example['original_image'])\n",
        "    pert_img = load_example_image(example['perturbed_image'])\n",
        "    \n",
        "    orig_resp = get_model_response(orig_img, example['original_question'])\n",
        "    pert_resp = get_model_response(pert_img, example['perturbed_question'])\n",
        "    \n",
        "    is_failure = example['expected_failure']\n",
        "    \n",
        "    display_example(orig_img, pert_img, example['original_question'], \n",
        "                   example['perturbed_question'], orig_resp, pert_resp,\n",
        "                   is_failure, example['perturbation_type'])\n",
        "    \n",
        "    equivalent_results.append({\n",
        "        'example_id': example['id'],\n",
        "        'is_failure': is_failure,\n",
        "        'original_response': orig_resp,\n",
        "        'perturbed_response': pert_resp\n",
        "    })"
      ],
      "metadata": {
        "id": "equivalent_examples"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run complementary examples\n",
        "complementary_results = []\n",
        "\n",
        "for i, example in enumerate(demo_config[\"complementary_examples\"], 1):\n",
        "    print(f\"üß™ COMPLEMENTARY EXAMPLE {i}: {example['dataset']}\")\n",
        "    \n",
        "    img = load_example_image(example['image'])\n",
        "    \n",
        "    orig_resp = get_model_response(img, example['original_question'])\n",
        "    comp_resp = get_model_response(img, example['complementary_question'])\n",
        "    \n",
        "    is_failure = example['expected_failure']\n",
        "    \n",
        "    display_complementary_example(img, example['original_question'],\n",
        "                                 example['complementary_question'],\n",
        "                                 orig_resp, comp_resp, is_failure, \"\")\n",
        "    \n",
        "    complementary_results.append({\n",
        "        'example_id': example['id'],\n",
        "        'is_failure': is_failure,\n",
        "        'original_response': orig_resp,\n",
        "        'complementary_response': comp_resp\n",
        "    })"
      ],
      "metadata": {
        "id": "complementary_examples"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary visualization\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "equiv_success = [0 if result['is_failure'] else 1 for result in equivalent_results]\n",
        "comp_success = [0 if result['is_failure'] else 1 for result in complementary_results]\n",
        "\n",
        "# Equivalent summary\n",
        "equiv_labels = ['Blur', 'Contrast', 'Noise', 'B&W', 'Masking']\n",
        "equiv_colors = ['green' if x == 1 else 'red' for x in equiv_success]\n",
        "\n",
        "bars1 = ax1.bar(equiv_labels, [1]*5, color=equiv_colors, alpha=0.7)\n",
        "ax1.set_title('EQUIVALENT Examples', fontweight='bold')\n",
        "ax1.set_ylabel('Success Rate')\n",
        "ax1.set_ylim(0, 1.2)\n",
        "\n",
        "for bar, success in zip(bars1, equiv_success):\n",
        "    status = '‚úÖ' if success else '‚ùå'\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
        "            status, ha='center', fontweight='bold', fontsize=16)\n",
        "\n",
        "# Complementary summary\n",
        "comp_labels = ['BLINK\\nSpatial 1', 'BLINK\\nSpatial 10', 'VSR\\nDog-Bike', 'VSR\\nHorse-Chair']\n",
        "comp_colors = ['blue' if x == 1 else 'red' for x in comp_success]\n",
        "\n",
        "bars2 = ax2.bar(comp_labels, [1]*4, color=comp_colors, alpha=0.7)\n",
        "ax2.set_title('COMPLEMENTARY Examples', fontweight='bold')\n",
        "ax2.set_ylabel('Success Rate')\n",
        "ax2.set_ylim(0, 1.2)\n",
        "\n",
        "for bar, success in zip(bars2, comp_success):\n",
        "    status = '‚úÖ' if success else '‚ùå'\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
        "            status, ha='center', fontweight='bold', fontsize=16)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('üé≠ FESTA Framework Results', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.show()\n",
        "\n",
        "# Stats\n",
        "total_equiv = sum(equiv_success)\n",
        "total_comp = sum(comp_success)\n",
        "overall = total_equiv + total_comp\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üìà FESTA RESULTS\")\n",
        "print(\"=\"*50)\n",
        "print(f\"üîÑ Equivalent: {total_equiv}/5 ({total_equiv/5*100:.0f}%)\")\n",
        "print(f\"üîÑ Complementary: {total_comp}/4 ({total_comp/4*100:.0f}%)\")\n",
        "print(f\"üìä Overall: {overall}/9 ({overall/9*100:.0f}%)\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "id": "summary_analysis"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
